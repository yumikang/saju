#!/usr/bin/env npx tsx
// ETL Step 3: ì¤‘ë³µ ì œê±° (Deduplication)
// ì •ê·œí™”ëœ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ë ˆì½”ë“œë¥¼ ì°¾ê³  ë³‘í•© ì „ëµì„ ì ìš©

import { join } from 'path';
import { 
  ETLConfig, 
  NormalizedData,
  MergedData,
  ProcessedHanjaRecord,
  ProcessingResult 
} from './lib/etl-types';
import { createLogger } from './lib/etl-logger';
import { 
  readJsonFile,
  writeJsonFile, 
  createProcessingResult, 
  createETLError,
  processBatches
} from './lib/etl-utils';

const STEP_NAME = '30_dedup';

// ê¸°ë³¸ ì„¤ì •
const config: ETLConfig = {
  inputDir: 'scripts/etl/data/normalized',
  outputDir: 'scripts/etl/data/merged',
  logLevel: 'info',
  batchSize: 100,
  errorHandling: 'continue',
  createBackup: true
};

/**
 * ì¤‘ë³µ ë ˆì½”ë“œ ê·¸ë£¹í™”
 */
function groupDuplicates(records: ProcessedHanjaRecord[]): Map<string, ProcessedHanjaRecord[]> {
  const groups = new Map<string, ProcessedHanjaRecord[]>();
  
  records.forEach(record => {
    const key = record.character.trim();
    if (!groups.has(key)) {
      groups.set(key, []);
    }
    groups.get(key)!.push(record);
  });

  return groups;
}

/**
 * ë ˆì½”ë“œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
 */
function calculateQualityScore(record: ProcessedHanjaRecord): number {
  let score = 0;
  
  // ê¸°ë³¸ ì ìˆ˜ (ì‹ ë¢°ë„ ê¸°ë°˜)
  score += (record.confidenceScore || 0.5) * 40;
  
  // í•„ë“œ ì™„ì„±ë„
  if (record.meaning && record.meaning.length > 0) score += 15;
  if (record.reading && record.reading.length > 0) score += 15;
  if (record.strokes && record.strokes > 0) score += 10;
  if (record.normalizedElement) score += 10;
  if (record.normalizedYinYang) score += 5;
  
  // ê²€ì¦ ìƒíƒœ ë³´ë„ˆìŠ¤
  if (record.validationStatus === 'ok') score += 5;
  
  return Math.min(100, score);
}

/**
 * ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„
 */
function getSourcePriority(source: string): number {
  const priorities: Record<string, number> = {
    'hanja-data.ts': 10,
    'supreme_court': 8,
    'external_api': 6,
    'user_input': 4,
    'unknown': 1
  };
  
  return priorities[source] || 1;
}

/**
 * ë‘ ë ˆì½”ë“œ ë³‘í•©
 */
function mergeRecords(
  primary: ProcessedHanjaRecord, 
  secondary: ProcessedHanjaRecord
): ProcessedHanjaRecord {
  const processingLog = [...(primary.processingLog || [])];
  processingLog.push(`Merged with record from ${secondary.source}`);
  
  // ë³‘í•© ë©”íƒ€ë°ì´í„° ìƒì„±
  const mergedMetadata = {
    ...primary.metadata,
    mergedSources: [
      primary.source || 'unknown',
      secondary.source || 'unknown'
    ],
    mergedAt: new Date().toISOString(),
    originalRecords: [primary.character, secondary.character]
  };

  return {
    character: primary.character,
    meaning: primary.meaning || secondary.meaning,
    reading: primary.reading || secondary.reading,
    strokes: primary.strokes || secondary.strokes,
    element: primary.element || secondary.element,
    yinYang: primary.yinYang || secondary.yinYang,
    normalizedElement: primary.normalizedElement || secondary.normalizedElement,
    normalizedYinYang: primary.normalizedYinYang || secondary.normalizedYinYang,
    source: primary.source,
    confidenceScore: Math.max(
      primary.confidenceScore || 0.5, 
      secondary.confidenceScore || 0.5
    ),
    validationStatus: primary.validationStatus === 'ok' && secondary.validationStatus === 'ok' 
      ? 'ok' 
      : 'needs_review',
    metadata: mergedMetadata,
    processingLog,
    duplicateInfo: {
      isDuplicate: false,
      duplicateOf: undefined,
      mergeStrategy: 'quality_based_merge'
    }
  };
}

/**
 * ì¤‘ë³µ ê·¸ë£¹ ë³‘í•©
 */
function mergeDuplicateGroup(
  duplicates: ProcessedHanjaRecord[], 
  groupKey: string,
  logger: any
): { merged: ProcessedHanjaRecord | null; errors: any[] } {
  const errors: any[] = [];
  
  try {
    if (duplicates.length === 1) {
      // ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°
      return { merged: duplicates[0], errors };
    }

    logger.debug(`Processing duplicate group for character: ${groupKey} (${duplicates.length} records)`);

    // í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    const scoredRecords = duplicates.map(record => ({
      record,
      qualityScore: calculateQualityScore(record),
      sourcePriority: getSourcePriority(record.source || 'unknown')
    }));

    // ì •ë ¬: í’ˆì§ˆ ì ìˆ˜ â†’ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ â†’ ì‹ ë¢°ë„ ì ìˆ˜ ìˆœ
    scoredRecords.sort((a, b) => {
      if (a.qualityScore !== b.qualityScore) {
        return b.qualityScore - a.qualityScore;
      }
      if (a.sourcePriority !== b.sourcePriority) {
        return b.sourcePriority - a.sourcePriority;
      }
      return (b.record.confidenceScore || 0.5) - (a.record.confidenceScore || 0.5);
    });

    // ìµœê³  í’ˆì§ˆ ë ˆì½”ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    let mergedRecord = scoredRecords[0].record;
    
    for (let i = 1; i < scoredRecords.length; i++) {
      mergedRecord = mergeRecords(mergedRecord, scoredRecords[i].record);
    }

    logger.debug(`Merged ${duplicates.length} records for ${groupKey} (final quality: ${calculateQualityScore(mergedRecord)})`);
    
    return { merged: mergedRecord, errors };

  } catch (error) {
    errors.push(createETLError(
      'processing',
      `Failed to merge duplicate group for ${groupKey}: ${error instanceof Error ? error.message : String(error)}`,
      duplicates,
      `group_${groupKey}`,
      undefined,
      { error }
    ));
    return { merged: null, errors };
  }
}

/**
 * ë°°ì¹˜ ì¤‘ë³µ ì œê±° ì²˜ë¦¬
 */
async function dedupBatch(
  groups: [string, ProcessedHanjaRecord[]][],
  batchIndex: number,
  logger: any
): Promise<{ merged: ProcessedHanjaRecord[], errors: any[] }> {
  const merged: ProcessedHanjaRecord[] = [];
  const errors: any[] = [];

  for (const [groupKey, duplicates] of groups) {
    const result = mergeDuplicateGroup(duplicates, groupKey, logger);
    
    if (result.merged) {
      merged.push(result.merged);
    }
    errors.push(...result.errors);
  }

  logger.debug(`Batch ${batchIndex}: processed ${groups.length} duplicate groups, ${merged.length} merged records`);
  
  return { merged, errors };
}

/**
 * ì¤‘ë³µ ì œê±° ë©”íŠ¸ë¦­ ê³„ì‚°
 */
function calculateMergeMetrics(
  originalCount: number,
  finalRecords: ProcessedHanjaRecord[],
  duplicateGroups: Map<string, ProcessedHanjaRecord[]>
) {
  let duplicatesFound = 0;
  let duplicatesRemoved = 0;
  const mergeStrategies: Record<string, number> = {};

  duplicateGroups.forEach((group, character) => {
    if (group.length > 1) {
      duplicatesFound += group.length;
      duplicatesRemoved += group.length - 1; // í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ì œê±°
    }
  });

  // ë³‘í•© ì „ëµ í†µê³„
  finalRecords.forEach(record => {
    const strategy = record.duplicateInfo?.mergeStrategy || 'no_merge';
    mergeStrategies[strategy] = (mergeStrategies[strategy] || 0) + 1;
  });

  return {
    originalCount,
    duplicatesFound,
    duplicatesRemoved,
    finalCount: finalRecords.length,
    mergeStrategies
  };
}

/**
 * ë©”ì¸ ì¤‘ë³µ ì œê±° í•¨ìˆ˜
 */
async function deduplicate(): Promise<ProcessingResult<MergedData>> {
  const logger = createLogger(config);
  const startTime = new Date();
  
  logger.startStep(STEP_NAME);

  try {
    // ì •ê·œí™”ëœ ë°ì´í„° ë¡œë“œ
    const inputPath = join(config.inputDir, 'normalized_data.json');
    logger.info(`Loading normalized data from: ${inputPath}`);
    
    const normalizedData = await readJsonFile<NormalizedData>(inputPath);
    logger.info(`Loaded ${normalizedData.records.length} normalized records`);

    // ì¤‘ë³µ ê·¸ë£¹í™”
    logger.info('Grouping duplicate records by character');
    const duplicateGroups = groupDuplicates(normalizedData.records);
    
    const totalGroups = duplicateGroups.size;
    const duplicateGroupCount = Array.from(duplicateGroups.values())
      .filter(group => group.length > 1).length;
    
    logger.info(`Found ${totalGroups} unique characters, ${duplicateGroupCount} duplicate groups`);

    // ë°°ì¹˜ë¡œ ì¤‘ë³µ ì œê±° ì²˜ë¦¬
    logger.info(`Processing duplicate groups in batches of ${config.batchSize}`);
    
    const allMerged: ProcessedHanjaRecord[] = [];
    const allErrors: any[] = [];

    const groupEntries = Array.from(duplicateGroups.entries());
    const batchResults = await processBatches(
      groupEntries,
      config.batchSize,
      async (batch, batchIndex) => {
        const result = await dedupBatch(batch, batchIndex, logger);
        allMerged.push(...result.merged);
        allErrors.push(...result.errors);
        
        logger.updateProgress(
          (batchIndex + 1) * config.batchSize,
          groupEntries.length,
          'Deduplicating records'
        );
        
        return result.merged;
      }
    );

    // ì¤‘ë³µ ì œê±° ë©”íŠ¸ë¦­ ê³„ì‚°
    const mergeMetrics = calculateMergeMetrics(
      normalizedData.records.length,
      allMerged,
      duplicateGroups
    );

    // ë³‘í•©ëœ ë°ì´í„° ê°ì²´ ìƒì„±
    const mergedData: MergedData = {
      records: allMerged,
      mergeMetrics
    };

    // ê²°ê³¼ ì €ì¥
    const outputPath = join(config.outputDir, 'merged_data.json');
    await writeJsonFile(outputPath, mergedData);

    // ì²˜ë¦¬ ê²°ê³¼ ìƒì„±
    const endTime = new Date();
    const result = createProcessingResult(
      mergedData,
      allMerged.length,
      allErrors,
      startTime,
      endTime
    );

    logger.endStep(STEP_NAME, result);
    logger.info(`Deduplication completed. Output saved to: ${outputPath}`);
    logger.info(`Removed ${mergeMetrics.duplicatesRemoved} duplicates from ${mergeMetrics.originalCount} records`);
    logger.info(`Final count: ${mergeMetrics.finalCount} unique records`);
    logger.info('Merge strategies:', mergeMetrics.mergeStrategies);

    return result;

  } catch (error) {
    const endTime = new Date();
    const errorObj = createETLError(
      'system',
      `Deduplication failed: ${error instanceof Error ? error.message : String(error)}`,
      undefined,
      undefined,
      undefined,
      { error }
    );

    const result = createProcessingResult(
      null,
      0,
      [errorObj],
      startTime,
      endTime
    );

    logger.endStep(STEP_NAME, result);
    logger.error('Deduplication failed', { error });
    throw error;
  }
}

// CLI ì‹¤í–‰ ì§€ì›
if (import.meta.url === `file://${process.argv[1]}`) {
  deduplicate()
    .then(result => {
      console.log('âœ… Deduplication completed successfully');
      console.log(`ğŸ“Š Processed: ${result.processedCount} records`);
      console.log(`âœ… Success: ${result.successCount} records`);
      console.log(`âŒ Errors: ${result.errorCount} errors`);
      console.log(`â±ï¸  Duration: ${result.metrics.processingTimeMs}ms`);
      
      if (result.data?.mergeMetrics) {
        const metrics = result.data.mergeMetrics;
        console.log('\nğŸ“ˆ Merge Metrics:');
        console.log(`  Original records: ${metrics.originalCount}`);
        console.log(`  Duplicates found: ${metrics.duplicatesFound}`);
        console.log(`  Duplicates removed: ${metrics.duplicatesRemoved}`);
        console.log(`  Final count: ${metrics.finalCount}`);
        console.log('  Merge strategies:', metrics.mergeStrategies);
      }
      
      process.exit(0);
    })
    .catch(error => {
      console.error('âŒ Deduplication failed:', error.message);
      process.exit(1);
    });
}

export { deduplicate };