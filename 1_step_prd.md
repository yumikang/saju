한자 데이터베이스 구축 전체 로드맵 


대법원 인명용 한자가 5천~8천 자 규모라면, 한 번에 가져오는 것보다 단계별로 확장하는 게 훨씬 안정적입니다.

⸻

📊 단계별 데이터 확장 전략

1단계 (약 1,000자) – 최소 MVP
	•	목표: 실제 서비스에서 바로 쓸 수 있는 최소 세트.
	•	소스:
	•	대법원 인명용 한자 중 가장 많이 쓰이는 성씨 + 이름용 상위 1천자
	•	(통계 기반 빈도순 or 대법원 표에서 자주 쓰이는 글자 리스트)
	•	작업:
	•	기존 236자 데이터와 통합
	•	표준화(오행·음양·획수) 규칙 적용
	•	JSON 변환 및 검증 체계 마련

👉 이 단계에서 데이터 파이프라인을 완전히 세팅하는 게 중요합니다.

⸻

2단계 (약 3,000자) – 중간 확장
	•	목표: 일반적인 작명/개명 서비스에서 커버 가능 수준.
	•	소스:
	•	대법원 인명용 한자 전체 공개본 (~3천자)
	•	필요시 국립국어원 사전 병합 (훈음·뜻 정리 보강)
	•	작업:
	•	1단계에서 만든 정규화/검증 파이프라인으로 대량 처리
	•	중복/충돌 검증 (needs_review 태깅)
	•	통계 기반 보강 (예: 가장 많이 쓰이는 오행 분포 확인)

👉 여기서부터 권위 소스 간 충돌이 나타나므로 conflict 관리 체계를 강화해야 합니다.

⸻

3단계 (5천자 이상) – Full Coverage
	•	목표: 대법원 인명용 한자 전체 (최대 8천자까지) 수용.
	•	소스:
	•	대법원 인명용 한자 전체판
	•	보조: Unicode Unihan 데이터 (부수, 획수, pinyin 등) → 속성 확장
	•	보조: 강희자전 or 전통 역학서적 (오행/음양 보정)
	•	작업:
	•	대량 병합 + needs_review 목록 정리
	•	전문가 검증(한자학/역학자) 또는 설명문 추가
	•	서비스 스펙에 맞게 alias(다중 읽기), search index 최적화

👉 최종 단계에서야 검색/추천/자동완성 같은 기능까지 얹을 수 있습니다.

⸻

📌 정리 – 단계별 기준
	1.	1단계: 최소 MVP (~1천자, 빈도 기반)
	2.	2단계: 중간 확장 (~3천자, 대법원 표 중간 규모)
	3.	3단계: 전체 확장 (5천8천자, full coverage + 외부 권위 소스)

⸻

즉,
	•	1단계는 “데이터 파이프라인 안정화 + 실제 사용 가능 최소셋”
	•	2단계는 “실무 서비스에서 대부분 커버”
	•	3단계는 “최종 백과사전급 DB 완성”

⸻


1단계: 최소 MVP(~1,000자, 빈도 기반) — 바로 오픈용

A) 데이터 준비
	•	빈도 리스트 확보: 성씨·이름에 자주 쓰이는 상위 한자 1,000자 목록(없으면 임시로 상위 성씨+작명 상위 한자 조합에서 뽑기)
	•	소스 적재: 기존 236자 + 상위 리스트를 data/raw/*.json으로 저장
	•	정규화: normalizeElement(→金木水火土), normalizeYinYang(→음/양) 적용
	•	다중 읽기 수집: 문자 기준 1행 + readings[](예: 星 → [‘성’, ‘별’])

B) 병합/충돌 처리
	•	문자 기준 dedup(메타 풍부한 쪽 우선)
	•	충돌 리졸버로 element 결정 + review=needs_review 태깅
	•	동일 키 내 동일 읽기 중복 제거(예: 率 ‘솔/솔’ 1건으로)

C) 스키마/시드
	•	HanjaDict(문자 1행), HanjaReading(읽기 테이블) 적용
	•	Prisma upsert 시드(idempotent) + ruleset/sourceTag 저장
	•	(선택) HanjaLegal에 인명용 여부/허용독음/이체자 추후 추가

D) 품질·검증
	•	문자 유니크 전/후 동일 테스트
	•	표준값만 존재(오행=金木水火土, 음양=음/양)
	•	needs_review 개수 스냅샷(대시보드로 모니터)
	•	샘플 100건 수동 점검(성씨×이름 2자 조합)

E) 런칭 최소 사양(Feature Flag = DB_MVP_V1)
	•	작명 엔진: 규칙 점수 결정적, LLM은 설명문만
	•	검색/자동완성: reading 기준 조회(별/성/임/림 등)
	•	금칙/부적절 필터 1차 적용
	•	결과 캐시(24h, Redis), 큐 우선순위(유료>무료)

MVP 성공 기준(OKR)
	•	추천 생성 성공률 ≥ 99%
	•	평균 응답 p95 ≤ 목표치
	•	에러·충돌 리포트 정상 생성
	•	needs_review 문구 마스킹 노출(오행 직접 표기 금지)

⸻

2단계: 중간 확장(~3,000자, 대법원 표 중간 규모)

A) 소스 확장/ETL
	•	대법원 표 중간 규모를 data/raw/scourt_mid.json으로 적재
	•	파서 → 정규화 → 병합 → 충돌처리 파이프라인 자동화
	•	sourceTag로 증분 로드(신규/변경/삭제 diff 리포트)

B) 권위도 보강(없으면 임시 가중치 유지)
	•	evidenceJSON 누적(소스별 가중치 테이블)
	•	리뷰 큐 운영(내부 콘솔에서 확정/코멘트)

C) 성능/운영
	•	배치 upsert 청크(예: 1,000건) + 트랜잭션
	•	인덱스 최적화: character 유니크, reading 인덱스, element 인덱스
	•	대시보드: 총 문자/오행 분포/평균 획수/needs_review 추이

⸻

3단계: 전체 확장(5천~8천자, Full Coverage + 외부 권위 소스)

A) 풀 데이터 적재
	•	대법원 전체판 ingest → 파이프라인 전량 처리
	•	보조 소스(예: Unihan, 사전)로 획수/부수/독음 보강
	•	전통 문헌/전문가 검수로 오행/음양 근거 강화

B) 검색·UX 고도화
	•	읽기별 음운오행 계산(syllable→soundElem)
	•	통계 기반 추천 가중치(usage/nameFrequency 보강)
	•	고급 필터(오행/획수/음양/성별감성/현대감성)

C) 안정화
	•	섀도우 리컴퓨팅(구버전 vs 신버전 결과 비교)
	•	회귀 테스트: 문자 유니크, 오행 분포 변화 임계치 알람
	•	법적 허용셋(HanjaLegal) 연동 완료 → 합법성 검사 자동화

⸻

폴더/스크립트(샘플 템플릿)

data/
  raw/            # 소스별 원본 JSON
  normalized/     # 정규화 결과(JSON)
  merged/         # 문자 기준 병합 + review 플래그
scripts/etl/
  10_fetch.ts
  20_parse_sourt.ts
  30_normalize.ts        // normalizeElement/YinYang
  40_dedup_merge.ts      // 문자 기준 병합 + readings 수집
  50_resolve_conflict.ts // needs_review, evidenceJSON
  60_validate.ts         // 계약검사(표준값/유실/중복)
  70_seed.ts             // Prisma upsert
  80_report.ts           // diff/통계/리뷰 큐 출력
app/lib/hanja/
  hanja.types.ts
  hanja.loader.ts
  hanja.repository.ts


⸻

운영 팁(핵심만)
	•	데이터=JSON, 코드=TS로 분리(관심사 분리)
	•	유일 경로 강제: DB 직전까지 normalize 필수
	•	전/후 문자 집합(Set) 비교로 유실 0 보장
	•	needs_review는 점수에 반영하되, 설명 노출 마스킹
	•	CI에서 카디널리티/분포/표준값 테스트로 회귀 차단

먼저 1단계용으로 20_parse_sourt.ts / 30_normalize.ts / 70_seed.ts 세 파일부터 만들면, 오늘 바로 MVP 데이터는 굴릴 수 있어요.