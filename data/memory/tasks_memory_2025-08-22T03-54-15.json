{
  "tasks": [
    {
      "id": "aa422f43-75b0-4310-bded-128269ef6761",
      "name": "Prisma 스키마 확장 및 마이그레이션",
      "description": "HanjaDict와 HanjaReading 모델을 contracts.md 규칙에 따라 확장하고, Element/YinYang/ReviewStatus enum 타입을 추가하여 데이터 무결성을 보장",
      "notes": "기존 데이터와의 호환성을 위해 nullable 필드로 설정. 마이그레이션 후 기존 데이터 검증 필요",
      "status": "completed",
      "dependencies": [],
      "createdAt": "2025-08-20T05:33:09.484Z",
      "updatedAt": "2025-08-20T05:38:28.562Z",
      "relatedFiles": [
        {
          "path": "prisma/schema.prisma",
          "type": "TO_MODIFY",
          "description": "스키마 확장 및 enum 타입 추가",
          "lineStart": 1,
          "lineEnd": 50
        }
      ],
      "implementationGuide": "1. schema.prisma에 enum Element { 金 木 水 火 土 }, enum YinYang { 음 양 }, enum ReviewStatus { ok needs_review } 추가\\n2. HanjaDict 모델 확장: element Element?, yinYang YinYang?, review ReviewStatus @default(ok), evidenceJSON String?, decidedBy String?, ruleset String?, codepoint Int?\\n3. HanjaReading 모델 신규 생성: character String, reading String, soundElem Element?, isPrimary Boolean @default(false), @@unique([character, reading])\\n4. 인덱스 추가: @@index([element]), @@index([reading])\\n5. npx prisma migrate dev 실행하여 마이그레이션 생성",
      "verificationCriteria": "마이그레이션 성공적으로 완료되고, enum 타입이 정상 생성되며, 기존 데이터가 손실 없이 유지됨",
      "analysisResult": "Phase 1 MVP 목표: 기존 236자 → 1,000자 확장을 위한 ETL 파이프라인 구축. 데이터 정규화, 중복 제거, 충돌 해결 시스템을 통해 운영 안정성을 확보하고, contracts.md 규칙을 준수하여 5,000자까지 확장 가능한 기반 마련",
      "summary": "Prisma 스키마 확장 성공적으로 완료: Element(金木水火土), YinYang(음양), ReviewStatus enum 타입 추가됨. HanjaDict 모델은 contracts.md 규칙에 따라 확장되었고, HanjaReading 모델이 새로 생성됨. 마이그레이션 성공적으로 실행되어 기존 데이터 손실 없이 스키마가 업데이트됨. enum 값은 @map 속성을 사용하여 CJK/한글 문자와 매핑됨.",
      "completedAt": "2025-08-20T05:38:28.562Z"
    },
    {
      "id": "57728999-353c-42fc-bd8b-d841593aead5",
      "name": "데이터 정규화 유틸리티 구현",
      "description": "contracts.md 규칙에 따른 normalizeElement, normalizeYinYang 함수를 구현하고, ValidationResult 패턴으로 안전한 변환 시스템 구축",
      "notes": "유일 경로 원칙 준수 - DB 진입 전 반드시 정규화 거쳐야 함. 에러 핸들링으로 배치 작업 중단 방지",
      "status": "completed",
      "dependencies": [
        {
          "taskId": "aa422f43-75b0-4310-bded-128269ef6761"
        }
      ],
      "createdAt": "2025-08-20T05:33:09.484Z",
      "updatedAt": "2025-08-20T05:46:25.128Z",
      "relatedFiles": [
        {
          "path": "app/lib/hanja-normalize.ts",
          "type": "CREATE",
          "description": "정규화 유틸리티 함수들",
          "lineStart": 1,
          "lineEnd": 100
        },
        {
          "path": "app/lib/hanja-enhanced.ts",
          "type": "TO_MODIFY",
          "description": "기존 정규화 함수 통합",
          "lineStart": 1,
          "lineEnd": 50
        }
      ],
      "implementationGuide": "1. app/lib/hanja-normalize.ts 파일 생성\\n2. normalizeElement 함수: 한글(토금수화목) → CJK(土金水火木) 변환, 매핑 테이블 사용\\n3. normalizeYinYang 함수: 陰/陽/음/양 → 음/양 변환\\n4. ValidationResult<T> 타입 정의: { success: boolean, data?: T, error?: string }\\n5. safeNormalizeElement, safeNormalizeYinYang 함수로 non-throwing 버전 제공\\n6. 단위 테스트 작성으로 모든 케이스 검증",
      "verificationCriteria": "모든 입력값이 표준 CJK/한글 형태로 정확히 변환되고, 잘못된 입력에 대해 적절한 에러 메시지 반환",
      "analysisResult": "Phase 1 MVP 목표: 기존 236자 → 1,000자 확장을 위한 ETL 파이프라인 구축. 데이터 정규화, 중복 제거, 충돌 해결 시스템을 통해 운영 안정성을 확보하고, contracts.md 규칙을 준수하여 5,000자까지 확장 가능한 기반 마련",
      "summary": "Prisma enum 호환 데이터 정규화 유틸리티를 성공적으로 구현했습니다. hanja-normalize.ts는 Element, YinYang, ReviewStatus에 대한 완전한 양방향 변환 시스템을 제공하며, 27개 단위 테스트가 모두 통과했습니다. contracts.md 규칙을 완전히 준수하고 ValidationResult 패턴으로 안전한 에러 처리를 보장합니다.",
      "completedAt": "2025-08-20T05:46:25.125Z"
    },
    {
      "id": "dbb53345-88d4-4b5b-9dc8-6575b05d204f",
      "name": "Supreme Court 한자 데이터 ingestion 스크립트 작성",
      "description": "10_ingest_gov.ts 스크립트를 작성하여 supreme-court-hanja.json을 읽고 표준 raw 포맷으로 변환",
      "status": "completed",
      "dependencies": [],
      "createdAt": "2025-08-20T06:56:57.167Z",
      "updatedAt": "2025-08-20T06:59:17.181Z",
      "relatedFiles": [
        {
          "path": "scripts/etl/data/raw/supreme-court-hanja.json",
          "type": "REFERENCE",
          "description": "다운로드된 Supreme Court 원본 데이터"
        },
        {
          "path": "scripts/etl/10_ingest_gov.ts",
          "type": "CREATE",
          "description": "새로 작성할 gov 데이터 ingestion 스크립트"
        },
        {
          "path": "scripts/etl/types.ts",
          "type": "REFERENCE",
          "description": "ETL 타입 정의"
        }
      ],
      "implementationGuide": "1. supreme-court-hanja.json 파일 읽기\n2. 각 레코드의 cd(hex code)를 실제 한자 문자로 변환\n3. ineum(한글 음), dic(의미) 등 필드 매핑\n4. 표준 RawHanjaRecord 형식으로 변환\n5. data/raw/gov-[timestamp].json으로 저장",
      "verificationCriteria": "10,163개 레코드가 올바르게 변환되어 raw JSON으로 저장됨",
      "analysisResult": "한국 인명용 한자 데이터(10,163자)를 ETL 파이프라인에 통합. 원본 데이터는 이미 다운로드됨(supreme-court-hanja.json). 기존 ETL 파이프라인 구조를 활용하여 단계별 처리.",
      "summary": "Supreme Court 한자 데이터 ingestion 스크립트를 성공적으로 작성하고 실행했습니다. 10,163개의 한자가 hex 코드에서 실제 문자로 변환되었고, 훈음에서 의미가 추출되어 표준 RawHanjaRecord 형식으로 저장되었습니다. 원본 데이터는 gov-2025-08-20T06-58-47-164Z.json 파일로 보존되었습니다.",
      "completedAt": "2025-08-20T06:59:17.180Z"
    },
    {
      "id": "1560cabe-8605-4d2c-8d28-79c197b2f1bd",
      "name": "한자-오행 룩업 테이블 생성",
      "description": "기존 185개 데이터에서 한자-오행 매핑을 추출하여 Gov 데이터에 적용할 룩업 테이블 생성",
      "notes": "기존 데이터 기반이므로 신뢰도 높음. Gov 데이터의 오행 매핑 기준으로 활용",
      "status": "completed",
      "dependencies": [],
      "createdAt": "2025-08-21T06:07:13.361Z",
      "updatedAt": "2025-08-21T06:10:49.619Z",
      "relatedFiles": [
        {
          "path": "scripts/etl/data/raw/raw_data.json",
          "type": "REFERENCE",
          "description": "기존 한자-오행 매핑 소스"
        },
        {
          "path": "scripts/etl/lib/hanja-element-lookup.ts",
          "type": "CREATE",
          "description": "생성할 룩업 테이블"
        },
        {
          "path": "app/lib/hanja-normalize.ts",
          "type": "REFERENCE",
          "description": "오행 매핑 유틸리티"
        }
      ],
      "implementationGuide": "1. scripts/etl/data/raw/raw_data.json에서 한자-오행 매핑 추출\\n2. Map<string, string> 형태로 룩업 테이블 생성\\n3. hanja-element-lookup.ts 파일로 저장\\n4. 검증: 185개 한자 모두 매핑되는지 확인",
      "verificationCriteria": "185개 한자-오행 매핑이 완전히 추출되고 검증됨",
      "analysisResult": "Gov 데이터(10,163자) ETL 정규화 및 중복 제거 단계 구현. 기존 ETL 파이프라인 확장을 통해 복수 읽기 처리, 오행/음양 매핑, 682개 중복 병합을 수행하여 10,000+ 한자 데이터베이스 구축",
      "summary": "한자-오행 룩업 테이블 성공적으로 생성. 기존 185개 raw 데이터에서 184개 유니크 한자의 오행 매핑을 추출하여 Map 구조로 구성. 오행별 분포: 목(42개), 수(41개), 금(38개), 화(36개), 토(27개). 조회, 검증, 통계 함수 포함하여 Gov 데이터 정규화에 활용 준비 완료.",
      "completedAt": "2025-08-21T06:10:49.618Z"
    },
    {
      "id": "22c75b63-3f1d-4e90-be1a-14d89e546210",
      "name": "20_normalize.ts Gov 데이터 처리 확장",
      "description": "기존 정규화 스크립트를 확장하여 Gov 데이터의 복수 읽기 분할 및 오행 매핑 처리",
      "notes": "기존 normalizeHanjaRecord 함수 확장, 호환성 유지",
      "status": "completed",
      "dependencies": [
        {
          "taskId": "1560cabe-8605-4d2c-8d28-79c197b2f1bd"
        }
      ],
      "createdAt": "2025-08-21T06:07:13.361Z",
      "updatedAt": "2025-08-21T06:32:12.860Z",
      "relatedFiles": [
        {
          "path": "scripts/etl/20_normalize.ts",
          "type": "TO_MODIFY",
          "description": "정규화 스크립트 확장"
        },
        {
          "path": "scripts/etl/lib/hanja-element-lookup.ts",
          "type": "DEPENDENCY",
          "description": "룩업 테이블 의존성"
        },
        {
          "path": "scripts/etl/lib/etl-types.ts",
          "type": "REFERENCE",
          "description": "타입 정의"
        }
      ],
      "implementationGuide": "1. Gov 데이터 감지 로직 추가 (source === 'supreme-court')\\n2. 복수 읽기 분할: reading.split(',').map(r => r.trim())\\n3. 첫 번째 읽기를 주 읽기로, 나머지는 alternativeReadings로 저장\\n4. 한자-오행 룩업 테이블 활용하여 element 매핑\\n5. Gov 메타데이터 보존 (hexCode, originalIndex, dictionaryInfo)",
      "verificationCriteria": "Gov 데이터의 복수 읽기가 올바르게 분할되고 오행이 매핑됨",
      "analysisResult": "Gov 데이터(10,163자) ETL 정규화 및 중복 제거 단계 구현. 기존 ETL 파이프라인 확장을 통해 복수 읽기 처리, 오행/음양 매핑, 682개 중복 병합을 수행하여 10,000+ 한자 데이터베이스 구축",
      "summary": "Gov 데이터 정규화 처리 확장 완료. 10,350개 레코드 성공적으로 정규화(기존 187개 + Gov 10,163개). 주요 기능: 1) 복수 읽기 분할 처리 - 1,566개 한자의 쉼표로 구분된 읽기를 주 읽기와 대안 읽기로 분할, 2) 한자-오행 룩업 테이블 활용 - 201개 한자에 기존 데이터 기반 오행 매핑 자동 적용, 3) Gov 메타데이터 보존 - hexCode, dictionaryInfo 등 원본 메타데이터 유지. 호환성을 유지하면서 기존 정규화 로직 확장 완료.",
      "completedAt": "2025-08-21T06:32:12.855Z"
    }
  ]
}